# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
This dataset contains data about applicants for some kind of bank offering. It looks like it is most likely a loan.
It contains personal data about the applicant (marriage status, age, housing, education etc) as well as financial info (if theyve had a loan before, if they've defaulted, the duration of their loan).
We seek to predict if an applicant will be approved or not based on these pieces of information.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
The best performing model was a 

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
For the SKLearn Hyperdrive pipeline architecture, we first took the raw data and put it into a Tabular dataset so it would be easier to manipulate and then we ran it through a clearning function that took
the information provided and one-hot encoded it to make it easier to see / use, primarily when it came to education level. We then split out the results of the application into their own column.

After I cleaned the data, we split it up into training and validation sets and then the training script set up logging for Accuracy as well as parameters for the run itself such as regularization strength
and max iterations at 1.0 and 100 respectively. The training script then ran a logistic regression on the training data and scored it against the validation set, logging the result as Accuracy. It also
established a place for models to be saved as well.

Now that the training script is complete, it was time to set up the SKLearn estimator and Hyperdrive configuration. The SKLearn estimator config was then run using the compute i created for this with train.py
as the entry script and the hyperdrive config was set up to maximize accuracy (logged from the training script) over 12 runs with 4 max concurrent runs at a time using random parameter sampling with a early 
termination BanditPolicy using evaluation_interval=2 and slack_factor=0.1.

**What are the benefits of the parameter sampler you chose?**
For learning rate, I chose normal(10,3) with 10 and 3 as my bounds because very large learnings rates result in unstable training and very small learning rates result in an inability to train. 
I tried running the same model with 20,2 as expanded bounds but saw little change in the results.
I went with random parameter sampling because it supports early termination of low performance runs which optimizes your time spent and its good for initial searches.

**What are the benefits of the early stopping policy you chose?**
The bandit policy is good because it will cancel any run where the primary metric is smaller than (Metric + Metric*Slack_Factor). This allows for early termination of low performance runs as well.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
